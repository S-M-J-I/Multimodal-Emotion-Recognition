{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lR0lWYje5kX"
   },
   "source": [
    "# **Emotion Recognition using Multimodal Deep Learning Approaches**\n",
    "\n",
    "In this experiment we propose a novel Multimodal Architecture to predict 8 different human emotions. We use audio and video as inputs to our model. The dataset this notebook runs on is the SAVEE datatset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibhyWildbSQ4",
    "outputId": "cb3f9153-3c07-4473-e9a2-4fa5dbdf0999"
   },
   "outputs": [],
   "source": [
    "!pip install av\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    !pip install torchsummary\n",
    "    from torchsummary import summary\n",
    "\n",
    "try:\n",
    "    from torcheval.metrics.functional import multiclass_f1_score\n",
    "except:\n",
    "    !pip install torcheval\n",
    "    from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image, read_video\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, Video\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gc\n",
    "\n",
    "# !pip install pytorchvideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize, ToTensor, ToPILImage, CenterCrop, ColorJitter, RandomPerspective\n",
    "\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA-xqGb8etrG"
   },
   "source": [
    "## **Setting up environment & hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wuLU2AIhesIg",
    "outputId": "258d8abc-fc76-4f38-ff3f-14134d6cd54d"
   },
   "outputs": [],
   "source": [
    "# Set up device: use GPU or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZbD7sUOb3HR",
    "outputId": "3964995e-4730-4834-95bf-06eb1e28949c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Uncomment this line if running from Google Colab\n",
    "\"\"\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2FgZoPCbl5p"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.cuda.get_device_name(0)\n",
    "except:\n",
    "    print(\"No CUDA. CPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbabBic1oIG8"
   },
   "outputs": [],
   "source": [
    "# The path to the root directory of the dataset. Change this on your system\n",
    "working_dir = \"/path_to_savee/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBwidu3dgOw0",
    "outputId": "e83b231b-c646-40d3-f256-477c1d80f195"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters. Tweak as you wish\n",
    "hyperparams = {\n",
    "    \"lr\": 0.0001, # Learning Rate\n",
    "    \"epochs\": 30, # Number of Epochs\n",
    "    \"adam_betas\": (0.98, 0.999), # B1 and B2 (weight decays) of ADAM\n",
    "    \"batch\": 16, # Mini-batch size\n",
    "    \"sdg_momentum\": 0.99, # Stochastic Gradient Descent momentum\n",
    "    \"sdg_weight_decay\": 0.45, # Stochastic Gradient Descent weight decay,\n",
    "}\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5IE6J8FgOuI"
   },
   "outputs": [],
   "source": [
    "# A dict that maps the class name to our assigned index (uses: track emotion index for prediction)\n",
    "\"\"\"\n",
    "This is for RAVDESS ONLY. Migrations for SAVEE and CREMA-D will be made later\n",
    "\"\"\"\n",
    "class2idx = {\n",
    "    \"anger\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happiness\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sadness\": 5,\n",
    "    \"surprise\": 6,\n",
    "}\n",
    "\n",
    "# A dict that maps the index to the class name (uses: decorate prediction)\n",
    "idx2class = {v:k for k,v in class2idx.items()}\n",
    "\n",
    "# A dict that maps the type given in the file name to our index(uses: dataset preparation)\n",
    "tag2idx = {\n",
    "    \"a\": 0,\n",
    "    \"d\": 1,\n",
    "    \"f\": 2,\n",
    "    \"h\": 3,\n",
    "    \"n\": 4,\n",
    "    \"sa\": 5,\n",
    "    \"su\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGYOIJQmgOic",
    "outputId": "284c8083-4c3c-46da-affd-acf63b1d915d"
   },
   "outputs": [],
   "source": [
    "idx2class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB2lBwgDmL4b"
   },
   "source": [
    "## **Defining the Transforms(Augmentation Techniques) and helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeTNYXx0lyTk"
   },
   "outputs": [],
   "source": [
    "# Defining the transforms:\n",
    "video_frame_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    Resize((252,252)),\n",
    "    CenterCrop((184,184)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# change frame color randomly\n",
    "video_frame_augment_color = Compose([\n",
    "    ToPILImage(),\n",
    "    Resize((252,252)),\n",
    "    CenterCrop((184,184)),\n",
    "    ColorJitter(brightness=0.4, hue=0.3, saturation=0.4),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# change frame prespective randomly\n",
    "video_frame_augment_persp = Compose([\n",
    "    ToPILImage(),\n",
    "    Resize((252,252)),\n",
    "    CenterCrop((184,184)),\n",
    "    RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHtHz0A7mgs6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Defining the helper functions for the Audio mel-spectogram technique\n",
    "\"\"\"\n",
    "\n",
    "# Get the melspec of the audio as image/np 2d array\n",
    "def wav2melSpec(AUDIO_PATH):\n",
    "    audio, sr = librosa.load(AUDIO_PATH)\n",
    "    return librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "\n",
    "# Show the image spectogram\n",
    "def imgSpec(ms_feature):\n",
    "    fig, ax = plt.subplots()\n",
    "    ms_dB = librosa.power_to_db(ms_feature, ref=np.max)\n",
    "    print(ms_feature.shape)\n",
    "    img = librosa.display.specshow(ms_dB, x_axis='time', y_axis='mel', ax=ax)\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    ax.set(title='Mel-frequency spectrogram');\n",
    "\n",
    "# Hear the audio\n",
    "def hear_audio(AUDIO_PATH):\n",
    "    audio, sr = librosa.load(AUDIO_PATH)\n",
    "\n",
    "    print(\"\\t\", end=\"\")\n",
    "    ipd.display(ipd.Audio(data=audio, rate=sr))\n",
    "\n",
    "\n",
    "def show_video(video_path):\n",
    "    from base64 import b64encode\n",
    "\n",
    "    if os.path.isfile(video_path):\n",
    "        ext = '.mp4'\n",
    "    else:\n",
    "        print(\"Error: Please check the path.\")\n",
    "\n",
    "    video_encoded = open(video_path, \"rb\").read()\n",
    "    data = \"data:video/mp4;base64,\" + b64encode(video_encoded).decode()\n",
    "\n",
    "    video_tag = '<video width=\"400\" height=\"300\" controls alt=\"test\" src=\"%s\">' % data\n",
    "    return HTML(data=video_tag)\n",
    "\n",
    "# Show 1 example\n",
    "def show_example(video_path, audio_path, prediction=None, actual=None, save_memory=False):\n",
    "    if prediction is not None:\n",
    "        print(\"Predicted Label:\", idx2class[prediction])\n",
    "    print(\"Actual Label:\", idx2class[actual])\n",
    "\n",
    "    if save_memory is False:\n",
    "        print(\"Video path:\", video_path)\n",
    "        ipd.display(Video(video_path, embed=True))\n",
    "\n",
    "        # display(show_video(video_path))\n",
    "        print(\"Audio path:\", audio_path)\n",
    "        hear_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SHmBEpnbl5y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Defining the helper functions for the Audio MFCC technique\n",
    "\"\"\"\n",
    "\n",
    "# audio effects\n",
    "def audio_effects(audio, sample_rate, augment=1):\n",
    "    data = None\n",
    "    if augment == 1:\n",
    "        data = librosa.effects.harmonic(y=audio)\n",
    "    elif augment == 2:\n",
    "        data = librosa.effects.pitch_shift(y=audio, sr=sample_rate, n_steps=3)\n",
    "    return data\n",
    "\n",
    "\n",
    "# normalize the audio wave\n",
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio\n",
    "\n",
    "def feature_extractor(file, augment=0, test=False):\n",
    "\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            data, sample_rate = librosa.load(file)\n",
    "            break\n",
    "        except:\n",
    "            if attempt == 50:\n",
    "                print(\"failed trying to find audio file\", file)\n",
    "                break\n",
    "            print(\"Audio file not read. Trying again\")\n",
    "            attempt += 1\n",
    "\n",
    "#     print(data.shape)\n",
    "\n",
    "    if augment > 0:\n",
    "        data = audio_effects(data, sample_rate, augment=augment)\n",
    "\n",
    "    data = normalize_audio(data)\n",
    "\n",
    "    # zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=data)[0]\n",
    "    zcr /= zcr.max()\n",
    "    zcr = zcr[0:(0+128)]\n",
    "    if len(zcr) < 128:\n",
    "        zcr = librosa.util.fix_length(zcr, size=128)\n",
    "#     result=np.vstack((result, zcr))\n",
    "\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=128).T, axis=0)\n",
    "    mfcc /= mfcc.max()\n",
    "#     result = np.vstack((result, mfcc))\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms /= rms.max()\n",
    "    rms = rms[0:(0+128)]\n",
    "    if len(rms) < 128:\n",
    "        rms = librosa.util.fix_length(rms, size=128)\n",
    "#     result = np.vstack((result, rms))\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = librosa.feature.melspectrogram(y=data, sr=sample_rate)\n",
    "    mel = librosa.amplitude_to_db(mel, ref = np.max)\n",
    "    mel = np.mean(mel.T, axis=0)\n",
    "    mel /= mel.sum()\n",
    "#     result = np.vstack((result, mel))\n",
    "\n",
    "    if test:\n",
    "        return_dict = {\n",
    "            \"raw\": data,\n",
    "            \"sr\": sample_rate,\n",
    "            \"zcr\": zcr,\n",
    "            \"mfcc\": mfcc,\n",
    "            \"rms\": rms,\n",
    "            \"mel\": mel\n",
    "        }\n",
    "    else:\n",
    "        return_dict = {\n",
    "            \"zcr\": zcr,\n",
    "            \"mfcc\": mfcc,\n",
    "            \"rms\": rms,\n",
    "            \"mel\": mel\n",
    "        }\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2iAzdWbbl5z"
   },
   "outputs": [],
   "source": [
    "def dict_to_tensor(dictionary):\n",
    "    out_dict = {}\n",
    "    for item in dictionary.items():\n",
    "        if item[0] == \"sr\":\n",
    "            out_dict[item[0]] = torch.tensor(item[1]).to(device)\n",
    "        else:\n",
    "            out_dict[item[0]] = torch.from_numpy(item[1]).to(device)\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3rsMw5Wj58W"
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae0Y6LRzi4AI"
   },
   "outputs": [],
   "source": [
    "# Make a tuple of audio, video, labels\n",
    "def make_ds_as_list(path):\n",
    "    audio = []\n",
    "    video = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in sorted(os.walk(f\"{path}Video\")):\n",
    "        for filename in sorted(filenames):\n",
    "            if filename == \"Info.txt\":\n",
    "                continue\n",
    "\n",
    "            video_path = os.path.join(dirname, filename)\n",
    "            label = re.split(\"(\\d+)\", filename)[0]\n",
    "            label = tag2idx[label]\n",
    "            video.append(video_path)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "    for dirname, _, filenames in sorted(os.walk(f\"{path}Audio\")):\n",
    "        for filename in sorted(filenames):\n",
    "            if filename == \"Info.txt\":\n",
    "                continue\n",
    "\n",
    "            audio_path = os.path.join(dirname, filename)\n",
    "            audio.append(audio_path)\n",
    "\n",
    "    return audio, video, labels\n",
    "\n",
    "# Create a dataframe\n",
    "def make_dataframe(path, augment=0):\n",
    "    audio, video, labels = make_ds_as_list(path)\n",
    "    data = pd.DataFrame()\n",
    "    data['audio_path'] = audio\n",
    "    data['video_path'] = video\n",
    "    data['label'] = labels\n",
    "    data['augment'] = augment\n",
    "\n",
    "    del audio\n",
    "    del video\n",
    "    del labels\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46gw50AkjxaM"
   },
   "outputs": [],
   "source": [
    "# Make the dataset\n",
    "df = make_dataframe(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8lyicr7nx6K"
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## **Dataset Splitting**\n",
    "\n",
    "We will now split the dataset into train, cv, test splits.\n",
    "\n",
    "60:20:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WVZ1JCxlNTC",
    "outputId": "787c6572-88e5-4ecb-c7ed-8cdf3e5ca97c"
   },
   "outputs": [],
   "source": [
    "# Split into 60% train, 20% val, 20% test set\n",
    "train_df, test_df = train_test_split(df, test_size=0.40, shuffle=True, random_state=42)\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_1_df = train_df.copy()\n",
    "augment_1_df[\"augment\"] = 1\n",
    "\n",
    "augment_2_df = train_df.copy()\n",
    "augment_2_df[\"augment\"] = 2\n",
    "\n",
    "train_df[\"augment\"] = 0\n",
    "\n",
    "# check an example to see if the strings/naming conventions match\n",
    "train_df[\"audio_path\"][0], train_df[\"video_path\"][0], augment_1_df[\"audio_path\"][0], augment_1_df[\"video_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, augment_1_df, augment_2_df])\n",
    "train_df.head()\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "fU2NCTHRlWX6",
    "outputId": "21e484f2-d26d-47a7-ddad-1656f86e0db8"
   },
   "outputs": [],
   "source": [
    "# # Check your examples\n",
    "# idx = 90 # Change index to see different examples\n",
    "# show_example(train_df[\"video_path\"].iloc[idx], train_df[\"audio_path\"].iloc[idx], actual=train_df[\"label\"].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiN_jVU4oHE_"
   },
   "outputs": [],
   "source": [
    "cv_df, test_df = train_test_split(test_df, test_size=0.50, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df['augment'] = 0\n",
    "test_df['augment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiF5Ib33oxsr",
    "outputId": "58336013-8a18-4a70-e9a7-7b5cd7a6b456"
   },
   "outputs": [],
   "source": [
    "# View their length\n",
    "len(train_df), len(cv_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXyeRaD2ucNK",
    "outputId": "09e34564-2cc8-48c0-f713-5a174fb2d7dd"
   },
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1Eq348OqFIc"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtHooQt9jzUo"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# The dataset to use in the dataloader\n",
    "class SAVEEDataset(Dataset):\n",
    "    def __init__(self, dataframe, video_frame_transform=None, video_strategy='optimal', cut_video=False, cut_audio=False):\n",
    "\n",
    "        self.cut_video = cut_video\n",
    "        self.cut_audio = cut_audio\n",
    "\n",
    "        self.examples = dataframe\n",
    "        self.video_frame_transform = {\n",
    "            0: video_frame_transform,\n",
    "            1: video_frame_augment_color,\n",
    "            2: video_frame_augment_persp\n",
    "        }\n",
    "\n",
    "\n",
    "        self.video_strategy = video_strategy\n",
    "\n",
    "        del dataframe, video_frame_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __optimal_strategy(self, video, augment=0):\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        q1_point = video.shape[0] // 4\n",
    "        q2_point = video.shape[0] // 2\n",
    "        q3_point = int((video.shape[0] * (3/4)))\n",
    "        length = video.shape[0]\n",
    "\n",
    "        q1_q2_mid = int((q1_point + (q2_point - 5))//2)\n",
    "        q2_q3_mid = int((q2_point + (q3_point - 5))//2)\n",
    "\n",
    "        q1_lb = q1_point-10\n",
    "        q1_up = q1_q2_mid\n",
    "\n",
    "        q2_lb = q2_point-10\n",
    "        q2_up = q2_q3_mid\n",
    "\n",
    "        q3_lb = q3_point-10\n",
    "\n",
    "        # select random starting frames\n",
    "        frames.append(self.video_frame_transform[augment](video[q1_lb]))\n",
    "        frames.append(self.video_frame_transform[augment](video[q1_point]))\n",
    "        frames.append(self.video_frame_transform[augment](video[q1_up]))\n",
    "\n",
    "        # select random mid frames\n",
    "        frames.append(self.video_frame_transform[augment](video[q2_lb]))\n",
    "        frames.append(self.video_frame_transform[augment](video[q2_point]))\n",
    "        frames.append(self.video_frame_transform[augment](video[q2_up]))\n",
    "\n",
    "        # select random end frames\n",
    "        frames.append(self.video_frame_transform[augment](video[q3_lb]))\n",
    "        frames.append(self.video_frame_transform[augment](video[q3_point]))\n",
    "        frames.append(self.video_frame_transform[augment](video[-1]))\n",
    "\n",
    "\n",
    "        frames = torch.stack(frames)\n",
    "\n",
    "        return frames\n",
    "\n",
    "\n",
    "    def __all_strategy(self, video):\n",
    "        length = video.shape[0]\n",
    "\n",
    "        frames = []\n",
    "        for i, f in enumerate(video):\n",
    "            if i == hyperparams[\"max_seq_len\"]:\n",
    "                break\n",
    "            frame = self.video_frame_transform(f)\n",
    "            frames.append(frame)\n",
    "            del frame\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        if length < hyperparams[\"max_seq_len\"]:\n",
    "            diff = hyperparams[\"max_seq_len\"] - length\n",
    "\n",
    "            Q2 = int(length // 2)\n",
    "            for i in range(Q2, Q2 + diff):\n",
    "                frames.append(self.video_frame_transform(video[i]))\n",
    "\n",
    "        frames = torch.stack(frames)\n",
    "        del video\n",
    "        gc.collect()\n",
    "\n",
    "        return frames\n",
    "\n",
    "\n",
    "    def __audio_extraction(self, audio, augment=0):\n",
    "        feats = feature_extractor(audio, augment)\n",
    "        return dict_to_tensor(feats)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = self.examples.iloc[idx]\n",
    "        audio_path, video_path, label, augment = df[\"audio_path\"], df[\"video_path\"], df[\"label\"], df['augment']\n",
    "\n",
    "        if self.cut_video == False:\n",
    "\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    video = read_video(video_path, pts_unit='sec')[0]\n",
    "                    break\n",
    "                except:\n",
    "                    if attempt == 50:\n",
    "                        print(\"Video file can't be read\", video_path)\n",
    "                        break\n",
    "                    print(\"Video file not read. Trying again\")\n",
    "                    attempt += 1\n",
    "\n",
    "\n",
    "            video = torch.permute(video, (0,3,1,2))\n",
    "\n",
    "            if self.video_strategy == 'optimal':\n",
    "                video = self.__optimal_strategy(video, augment)\n",
    "            elif self.video_strategy == 'all':\n",
    "                video = self.__all_strategy(video)\n",
    "        else:\n",
    "            video = torch.zeros((1,1))\n",
    "\n",
    "        if self.cut_audio == False:\n",
    "            audio = self.__audio_extraction(audio_path, augment)\n",
    "        else:\n",
    "            audio = torch.zeros((1,1))\n",
    "\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "        return video, audio, label, video_path, audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PD7KdpAjzR0"
   },
   "outputs": [],
   "source": [
    "trainds = SAVEEDataset(train_df, video_frame_transform, video_strategy='optimal')\n",
    "cvds = SAVEEDataset(cv_df, video_frame_transform, video_strategy='optimal')\n",
    "testds = SAVEEDataset(test_df, video_frame_transform, video_strategy='optimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O34tXEQvjzO3"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainds, batch_size=hyperparams[\"batch\"], shuffle=True)\n",
    "cvloader = DataLoader(cvds, batch_size=hyperparams[\"batch\"], shuffle=False)\n",
    "testloader = DataLoader(testds, batch_size=hyperparams[\"batch\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tO-T4pmUbl5_",
    "outputId": "c1da65e0-2e04-4598-c568-ebb5456a3879"
   },
   "outputs": [],
   "source": [
    "del trainds\n",
    "del cvds\n",
    "del testds\n",
    "del train_df\n",
    "del cv_df\n",
    "del test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3WEdO8OrGE7"
   },
   "source": [
    "## **The Model**\n",
    "\n",
    "It's time to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A1UqmQcbl6T"
   },
   "source": [
    "### The Video Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3AfeLqtbl6U",
    "outputId": "14b56cc9-10bc-40cd-8da8-94a02bcfd83d"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.video import r2plus1d_18\n",
    "\n",
    "R2plus1D = r2plus1d_18(weights='KINETICS400_V1').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTAtuAoUbl6V"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.video.resnet import Conv2Plus1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4guMEY8bl6W"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "### The Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa8NMi1rbl6Y"
   },
   "outputs": [],
   "source": [
    "class PassThrough(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blyDHcAhbl6b"
   },
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=3, dilation=2, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.GroupNorm(1, 128)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.GroupNorm(1, 128)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.GroupNorm(1, 128)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqO8d9FqZegd"
   },
   "outputs": [],
   "source": [
    "class SENet(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.se_net = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.global_pooling_bridge = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.global_pooling_bridge(x)\n",
    "#         print(\"shape input to se net: \", out.shape)\n",
    "        out = self.flatten(out)\n",
    "        out = self.se_net(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu5yeD7GZfPx"
   },
   "outputs": [],
   "source": [
    "class DaggerNetV2(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_encoder = Conv1D()\n",
    "\n",
    "        self.se_net = SENet(channels=128)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.global_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.cnn_encoder(x)\n",
    "        residual = out\n",
    "\n",
    "\n",
    "        attn_out = self.se_net(out)\n",
    "\n",
    "        attn_out = attn_out.unsqueeze(dim=-1)\n",
    "\n",
    "        out_total = attn_out * residual\n",
    "\n",
    "        out_total = self.flatten(out_total)\n",
    "\n",
    "        return out_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFi9e5M7bl6c"
   },
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor(nn.Module):\n",
    "    def __init__(self, rnn_hidden_size, rnn_num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "\n",
    "\n",
    "        self.zcr_net = DaggerNetV2()\n",
    "        self.rms_net = DaggerNetV2()\n",
    "        self.mfcc_net = DaggerNetV2()\n",
    "        self.mel_net = DaggerNetV2()\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x[\"mfcc\"] = x[\"mfcc\"].unsqueeze(dim=1).float()\n",
    "        x[\"zcr\"] = x[\"zcr\"].unsqueeze(dim=1).float()\n",
    "        x[\"mel\"] = x[\"mel\"].unsqueeze(dim=1).float()\n",
    "        x[\"rms\"] = x[\"rms\"].unsqueeze(dim=1).float()\n",
    "\n",
    "\n",
    "\n",
    "        out_mfcc = self.mfcc_net(x[\"mfcc\"])\n",
    "        out_mel = self.mel_net(x[\"mel\"])\n",
    "\n",
    "        out_zcr = self.zcr_net(x[\"zcr\"])\n",
    "        out_rms = self.rms_net(x[\"rms\"])\n",
    "\n",
    "\n",
    "        combined = torch.cat([out_mfcc, out_zcr, out_rms, out_mel], dim=1)\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfcc-gTdbl6d"
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "## The Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV5HuO1ZkMLr"
   },
   "outputs": [],
   "source": [
    "class MainMultimodal(nn.Module):\n",
    "    def __init__(self, num_classes, trainable=False, fine_tune_limit=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "#         # define video extractor, cut off FCN layer\n",
    "        self.video_extractor = R2plus1D\n",
    "\n",
    "#         # cut off layer fcn\n",
    "        self.video_extractor.fc = PassThrough()\n",
    "\n",
    "\n",
    "        # define audio extractor\n",
    "        self.audio_extractor = AudioFeatureExtractor(rnn_hidden_size=32, rnn_num_layers=1).to(device)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(512 + 5632),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512 + 5632, num_classes, bias=True),\n",
    "        )\n",
    "\n",
    "        # init dual gpu usuage\n",
    "        self.video_extractor = nn.DataParallel(self.video_extractor)\n",
    "        self.audio_extractor = nn.DataParallel(self.audio_extractor)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            Set the model to trainable false\n",
    "        \"\"\"\n",
    "        if trainable is False:\n",
    "            for param in self.video_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "\n",
    "            \"\"\"\n",
    "                Train all layers\n",
    "            \"\"\"\n",
    "            if fine_tune_limit == 'all':\n",
    "                for param in self.video_extractor.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            else:\n",
    "                \"\"\"\n",
    "                    Set the fine tune limits\n",
    "                \"\"\"\n",
    "                count = 0 # keep track for layer count\n",
    "                length = sum(1 for _ in self.video_extractor.module.children()) # get the length of layers\n",
    "                limit = length - fine_tune_limit # set the limit [if length is 7, then limit = 7-2(default) = 5 ---> if count is = or above this we set to trainable ]\n",
    "\n",
    "\n",
    "                for child in self.video_extractor.module.children():\n",
    "                    if count >= limit:\n",
    "                        for param in child.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in child.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "\n",
    "        video = torch.permute(video, (0,2,1,3,4))\n",
    "\n",
    "\n",
    "        video_feature_values = self.video_extractor(video)\n",
    "\n",
    "        audio_feature_values = self.audio_extractor(audio)\n",
    "\n",
    "\n",
    "        del video, audio\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        combined = torch.cat([video_feature_values, audio_feature_values], dim=1)\n",
    "\n",
    "        out_logits = self.fc(combined)\n",
    "        out_softmax = self.softmax(out_logits)\n",
    "\n",
    "\n",
    "        return out_logits, out_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cwFz_w-JrG4"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "modelV1 = MainMultimodal(len(idx2class), trainable=True, fine_tune_limit=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82mWAMMGbl6f",
    "outputId": "1086bdcb-6c9a-4f17-ce4e-8ec9889c870a"
   },
   "outputs": [],
   "source": [
    "modelV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njefzEopbl6g"
   },
   "outputs": [],
   "source": [
    "# next(modelV1.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hh67CLS0KP4p"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(params=modelV1.parameters(), lr=hyperparams[\"lr\"], betas=hyperparams[\"adam_betas\"], weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZaA9yP7J6DE"
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, dataloader, optimizer, loss_fn, accuracy_fn=None, save_memory=False):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (videos, audios, labels, video_paths, audio_paths) in enumerate(dataloader):\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        y_logits, y_softmax = model(videos, audios)\n",
    "        y_logits, y_softmax = y_logits.to(device), y_softmax.to(device)\n",
    "\n",
    "        # print(y_logits.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = y_softmax.argmax(dim=1).to(device)\n",
    "        videos = videos.detach().cpu()\n",
    "        # audios = audios.detach().cpu()\n",
    "        del videos, audios\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        # print(labels.shape, preds.shape)\n",
    "\n",
    "        loss = loss_fn(y_logits, labels)\n",
    "        acc = accuracy_fn(preds, labels, num_classes=len(idx2class))\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if batch == 0 or batch == len(dataloader) - 1:\n",
    "            sample = random.randint(1, y_logits.shape[0])-1\n",
    "            print(f\"Batch: #{batch} | Train Loss: {loss} | Train Accuracy: {acc}\")\n",
    "            show_example(video_paths[sample], audio_paths[sample], preds[sample].detach().cpu().item(), labels[sample].detach().cpu().item(), save_memory)\n",
    "\n",
    "        del labels\n",
    "        del video_paths\n",
    "        del audio_paths\n",
    "        preds = preds.detach().cpu()\n",
    "        del preds\n",
    "        y_logits = y_logits.detach().cpu()\n",
    "        del y_logits\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    print(f\"Total Train loss: {train_loss} | Total Train accuracy: {train_acc}\")\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def eval_step(model: torch.nn.Module, dataloader, loss_fn, accuracy_fn=None, save_memory=False, confusion_matrix=False):\n",
    "    eval_loss = 0.0\n",
    "    eval_acc = 0.0\n",
    "\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (videos, audios, labels, video_paths, audio_paths) in enumerate(dataloader):\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "            y_logits, y_softmax = model(videos, audios)\n",
    "            y_logits, y_softmax = y_logits.to(device), y_softmax.to(device)\n",
    "\n",
    "            preds = y_softmax.argmax(dim=1).to(device)\n",
    "\n",
    "            if confusion_matrix:\n",
    "                y_preds.extend(preds.detach().cpu().numpy())\n",
    "                y_true.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            videos = videos.detach().cpu()\n",
    "            # audios = audios.detach().cpu()\n",
    "            del videos, audios\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            loss = loss_fn(y_logits, labels)\n",
    "            acc = accuracy_fn(preds, labels, num_classes=len(idx2class))\n",
    "            eval_loss += loss.item()\n",
    "            eval_acc += acc\n",
    "\n",
    "\n",
    "            if batch == 0 or batch == len(dataloader) - 1:\n",
    "                sample = random.randint(1, y_logits.shape[0])-1\n",
    "                print(f\"Batch: #{batch} | Eval. Loss: {loss} | Eval. Accuracy: {acc}\")\n",
    "                show_example(video_paths[sample], audio_paths[sample], preds[sample].detach().cpu().item(), labels[sample].detach().cpu().item(), save_memory)\n",
    "\n",
    "            del labels\n",
    "            del video_paths\n",
    "            del audio_paths\n",
    "            preds = preds.detach().cpu()\n",
    "            del preds\n",
    "            y_logits = y_logits.detach().cpu()\n",
    "            del y_logits\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        eval_loss /= len(dataloader)\n",
    "        eval_acc /= len(dataloader)\n",
    "\n",
    "    print(f\"Total Eval. Loss: {eval_loss} | Total Eval. Accuracy: {eval_acc}\")\n",
    "\n",
    "    if confusion_matrix:\n",
    "        return eval_loss, eval_acc, y_true, y_preds\n",
    "    else:\n",
    "        return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFW2EX8Nbl6j",
    "outputId": "efbb9444-8d3c-4e97-85d0-b127f95ece47"
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWfBz3SBbl6j"
   },
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_loss_history = []\n",
    "eval_loss_history = []\n",
    "\n",
    "train_accuracy_history = []\n",
    "eval_accuracy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCb7QSi5bl6k"
   },
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "best_train_loss, best_eval_loss = 10000, 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x36S97zpL1Ak"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "save_memory = True\n",
    "\n",
    "if save_memory:\n",
    "    print(\"\\tSave memory mode is on. Set `save_memory=False` to see video-audio examples\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(hyperparams[\"epochs\"]):\n",
    "    print(f\"========================== Starting Epoch: # {epoch} ==========================\")\n",
    "\n",
    "    inference_start = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_step(modelV1, trainloader, optim, loss_fn, multiclass_f1_score, save_memory=save_memory)\n",
    "    eval_loss, eval_acc = eval_step(modelV1, cvloader, loss_fn, multiclass_f1_score, save_memory=save_memory)\n",
    "\n",
    "    inference_total = time.time() - inference_start\n",
    "\n",
    "\n",
    "    print(f\"Epoch: #{epoch} | Total Train Loss: {train_loss} | Total Eval. Loss: {eval_loss} | Train Acc: {train_acc * 100}% | Eval Acc: {eval_acc * 100}% in {inference_total} seconds\")\n",
    "\n",
    "\n",
    "    epochs.append(epoch+1)\n",
    "    train_loss_history.append(train_loss)\n",
    "    eval_loss_history.append(eval_loss)\n",
    "    train_accuracy_history.append(train_acc.detach().cpu()*100)\n",
    "    eval_accuracy_history.append(eval_acc.detach().cpu()*100)\n",
    "\n",
    "    if train_loss < best_train_loss and eval_loss < best_eval_loss:\n",
    "        best_train_loss, best_eval_loss = train_loss, eval_loss\n",
    "        torch.save(modelV1.state_dict(), \"./best-multimodal.pt\")\n",
    "        best_w = modelV1.state_dict()\n",
    "\n",
    "    del train_loss, eval_loss, train_acc, eval_acc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "total = end - start\n",
    "convert = str(datetime.timedelta(seconds=total))\n",
    "print(f\"Total Training Time: {total}s => {convert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRIwKJd2bl6l"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL7zVWAJKoZA"
   },
   "outputs": [],
   "source": [
    "torch.save(modelV1.state_dict(), \"./multimodal-final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Prr5Tsibl6m"
   },
   "outputs": [],
   "source": [
    "# epoch = hyperparams[\"epochs\"]\n",
    "epoch = len(epochs)\n",
    "\n",
    "plt.plot(epochs, train_loss_history, color='dodgerblue', label='Train Loss')\n",
    "plt.plot(epochs, eval_loss_history, color='orange', label='Eval. Loss')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.title(f\"Train and Eval. Loss along {epoch} epochs (SAVEE)\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"./Loss curves.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLHCcmpBbl6n"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, train_accuracy_history, color='dodgerblue', label='Train Accuracy')\n",
    "plt.plot(epochs, eval_accuracy_history, color='orange', label='Eval. Accuracy')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 Score Value\")\n",
    "plt.title(f\"Train and Eval. Accuracy along {epoch} epochs (SAVEE)\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"./F1-Score curves.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdr8tCLIbl6p"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for params in modelV1.parameters():\n",
    "    count +=1\n",
    "\n",
    "print(f\"There are {count} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yITmVaVVXcYl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "CcqkbFj4bl6p",
    "outputId": "1a69f39a-a482-42c4-ae1c-f73868908fc4"
   },
   "outputs": [],
   "source": [
    "# load best weights model\n",
    "modelV1.load_state_dict(torch.load('./best-multimodal.pt'))\n",
    "\n",
    "\n",
    "test_loss, test_acc, y_true, y_preds = eval_step(modelV1, testloader, loss_fn, multiclass_f1_score, save_memory=False, confusion_matrix=True)\n",
    "test_acc = test_acc.detach().cpu()\n",
    "\n",
    "print(f\"Test loss: {test_loss}\\tTest Accuracy: {test_acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "SY8Avtn4XOXV",
    "outputId": "14a26195-dab8-4ddf-d872-5bc5814f173b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_true, y_preds, target_names=[v for k,v in idx2class.items()], output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "df = df.round(decimals=4)\n",
    "\n",
    "df.to_csv('./classification_report.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "M7Gd_x1VuESp",
    "outputId": "461de6f4-decf-4b4e-e2d5-c404443ad4a7"
   },
   "outputs": [],
   "source": [
    "classes = [v for k,v in idx2class.items()]\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_preds)\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes], columns = [i for i in classes])\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "\n",
    "s = sn.heatmap(df_cm, annot=True, cmap='Blues', fmt=\".2f\")\n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=14, labelpad=20, fontweight='bold')\n",
    "\n",
    "plt.ylabel('True Label', fontsize=14, labelpad=20, fontweight='bold')\n",
    "\n",
    "# format_margins(s, x=0.1)\n",
    "\n",
    "plt.savefig('./confusion_matrix_savee.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2fHx3QvXL5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPH3Ija-bl6q"
   },
   "outputs": [],
   "source": [
    "# # Save stats\n",
    "# with open(\"./recorded.txt\", \"w\") as f:\n",
    "#     f.write(\"R2plus1D & VGG1D-BiLSTM attempt\\n\")\n",
    "#     for i, line in enumerate(epochs):\n",
    "#         f.write(f\"Epoch: {line}: | Train Loss: {train_loss_history[i]} | Train Accuracy: {train_accuracy_history[i]} | Eval Loss: {eval_loss_history[i]} | Eval Accuracy: {eval_accuracy_history[i]}\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "#     f.write(\"\\n==================================================\\n\")\n",
    "#     f.write(f\"On best weights => Test loss: {test_loss}\\tTest Accuracy: {test_acc*100}\")\n",
    "#     f.write(\"\\n==================================================\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "379Pvomcfxas"
   },
   "source": [
    "## **Citations**\n",
    "\n",
    "1. Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
